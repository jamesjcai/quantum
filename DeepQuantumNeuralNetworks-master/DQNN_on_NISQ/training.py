# Own modules
import save_data as sd
from network_classes import Network_DQNN
from network_classes import Network_QAOA

# User configuration
from user_config import provider_info

# General imports
from copy import deepcopy
from time import time

# Avoid thousands of INFO logging lines
import logging
import qiskit # type: ignore
qiskit.transpiler.passes.basis.basis_translator.logger.setLevel(logging.ERROR)
qiskit.transpiler.runningpassmanager.logger.setLevel(logging.ERROR)

# ---- QISKIT ----
from qiskit import QuantumCircuit, Aer, QuantumRegister, ClassicalRegister, IBMQ
from qiskit.quantum_info.states.utils import partial_trace # type: ignore
from qiskit.quantum_info.random import random_unitary, random_statevector # type: ignore
from qiskit.providers.ibmq import least_busy # type: ignore
from qiskit.circuit.library.standard_gates import U3Gate, RXGate, RYGate, XGate # type: ignore
from qiskit.providers.aer import QasmSimulator # type: ignore

# additional math libs
import numpy as np # type: ignore
from scipy.constants import pi # type: ignore
from scipy.linalg import expm # type: ignore

# Typing
from typing import Union, Optional, List, Tuple, Any

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# global device variable for simulator=True
DEVICE = None

def generate_circuits(network: Union[Network_DQNN, Network_QAOA],
                      all_params: Any = [],
                      circuits_type: str = "id_circuits",
                      shots: int = 2**13,
                      draw_circ: bool = False) -> dict:
    """
    Generates a dictionary including executable circuits given a network type,
    the parameters, and some other optionable properties.

    Args:
        network (Union[Network_DQNN, Network_QAOA]): Network class object.
        all_params (Any, optional): 2-dimensional with 1-d lists of network parameters. Defaults to [].
        circuits_type (str, optional): Type of circuit, depending on the circuits' use case. Defaults to "id_circuits".
        shots (int, optional): Number of shots. Automatically multiplies circuits if (shots > 2^13). Defaults to 2^13.
        draw_circ (bool, optional): Set if the transpiled circuit should be saved as a png file. Defaults to False.

    Returns:
        dict: Including the properties circuits_type, ap_circuits (with assigned parameters),
              num_states (e.g. 4 training pairs), and loops (e.g. 2 for 2^14 shots)
              This dictionary can be fed into execute_circuits()
    """    
    
    # Check and if necessary correct shape of parameter list
    if len(np.shape(all_params)) != 2:
        all_params = [all_params]
    
    # Helper evariable1s that enables to execute circuits with shots > 2^14
    loop, shots = number_of_loops(shots)
    
    # Get parametrized circuit (parameters not assigned yet)
    circuits = getattr(network, circuits_type)
    circuits = circuits if isinstance(circuits, list) else [circuits]
    
    # Define parameters for identity circuit
    if circuits_type == "id_circuits":
        all_params = [network.identity_network_parameters()]
    
    # Generate circuits with assigned parameters
    ap_circuits: List[QuantumCircuit] = []
    if circuits_type == "fid_circuits":
        # No parameters as the fidelity circuit consist of the fidelity measurement only
        ap_circuits = circuits * loop
    else:
        for network_params in all_params:
            for circuit in circuits: # loop over training/validation pairs
                for _ in range(loop):
                    ap_circuits.append(circuit.assign_parameters({network.param_vector: network_params}))
            
    if draw_circ:
        sd.draw_circuit(ap_circuits[0], filename='transpiled_{}.png'.format(circuits_type))
    
    return {'circuits_type': circuits_type, 'circuits': ap_circuits, 'num_states': len(circuits), 'loops': loop}

def execute_circuits(network: Union[Network_DQNN, Network_QAOA],
                     circuits: List[Any],
                     simulator: bool,
                     device_name: str,
                     epoch: int = 0,
                     shots: int = 2**13) -> List[List[float]]:
    """
    Executes a single or a list of circuits on either the
    simulator or a real device with the given number of shots.

    Args:
        network (Union[Network_DQNN, Network_QAOA]): Network class object.
        circuits (List[Any]): List of dictionaries including executable circuits. Generated by generate_circuits().
        simulator (bool): Tells the function if a simulator is used or a real device
        device_name (str): Choose an execution device by name.
        epoch (int, optional): Epoch of learning, used to identify calibrations. Defaults to 0.
        shots (int, optional): Number of times a single circuit is run. Defaults to 2^13.

    Returns:
        List[List[float]]: Returns the averaged cost for every different parameter set in the following form [[training_cost_old, paramset_1_training, paramset_2_training,...], [validation_cost], [identity_cost], [fidelity_cost]]
    """
        
    # Check and if necessary correct shape of circuit dict list
    if not isinstance(circuits[0], dict):
        circuits = [circuits]
    
    # Define device
    if simulator:
        # Fix device to first calibration
        global DEVICE
        device = DEVICE or get_device(network, simulator, epoch, device_name=device_name)
        DEVICE = device
    else:
        # Get device and download calibration data for each epoch
        device = get_device(network, simulator, epoch, device_name=device_name)
    
    # Helper variable that enables to execute circuits with shots > 2^14
    shots = number_of_loops(shots)[1]
    
    # Flatten the circuits but remember the shape -> Execute different circuit types within one request
    shape: List[int] = []
    flat_circuits: List[QuantumCircuit] = []
    for k, circ in enumerate(circuits):
        shape.append(shape[k-1]+len(circ.get('circuits')) if k > 0 else len(circ.get('circuits')))
        flat_circuits.extend(circ.get('circuits'))
    shape = shape[:-1] # Remove last index for better use with np.split()
    
    # Execute all circuits on device
    counts = network.execute_circuits(flat_circuits, device, shots)
    # ==================================================================
    # Replace the above line with the following if your Internet 
    # connection is unstable. The job will be resubmitted if an  
    # error occurs during execution.                             
    # ------------------------------------------------------------------
    # counts = None
    # while counts is None:
    #     try:
    #         counts = network.execute_circuits(flat_circuits, device, shots)
    #     except:
    #         logger.warning("An error occured executing your job.")
    # ==================================================================
    
    # Post-processing of the measurement results
    if not isinstance(counts, list): counts = [counts]
    if network.fid_meas_method == 'destructive_swap':
        flat_costs = get_cost_from_counts(network, counts, shots)
    else:
        # Big swap test
        c0 = np.asarray([count.get('0', 0) for count in counts])
        c1 = np.asarray([count.get('1', 0) for count in counts])
        flat_costs = (c0-c1)/shots 
        
    # Sort costs into list for different circuit types
    avg_costs = [[np.average(c) for c in np.split(costs,len(costs)//(circuits[i].get('loops') * circuits[i].get("num_states")))] for i, costs in enumerate(np.split(flat_costs, shape))]
    avg_costs_sorted = []
    circuits_type_inputs = [circ.get("circuits_type") for circ in circuits]
    for circuits_type in ['tp_circuits', 'vp_circuits', 'id_circuits', 'fid_circuits']:
        i = circuits_type_inputs.index(circuits_type) if circuits_type in circuits_type_inputs else -1
        avg_costs_sorted.append(avg_costs[i] if i > -1 else [np.nan])

    return avg_costs_sorted

def get_cost_from_counts(network: Union[Network_DQNN, Network_QAOA],
                         counts: List[dict],
                         shots: int) -> List[float]:
    """
    Classical post-processing to evaluate fidelity from the destructive swap's measurement.

    Args:
        network (Union[Network_DQNN, Network_QAOA]): Network class object.
        counts (List[dict]): Counts as returned from network.execute_circuits().
        shots (int): Actually used Number of shots.

    Returns:
        List[float]: Corresponding costs to counts.
    """    
    c01 = np.zeros((2,len(counts)), dtype=int)
    
    # Calulate fidelity
    # counts looks like [{"01": 1024, "11": 1023, "00": 1}, ...] with one dict for every circuit
    for j, count in enumerate(counts):
        for state, c in count.items():
            input_state_vec = np.asarray(list(map(int, state[:len(state)//2])))
            output_state_vec = np.asarray(list(map(int, state[len(state)//2:])))
            res = np.dot(input_state_vec, output_state_vec) % 2
            c01[res][j] += c
    return (c01[0]-c01[1])/shots

def get_provider() -> Any:
    """
    Get IBMQ provider as defined in user_config. Fallback if user has no access.

    Returns:
        Any: IBMQ provider
    """
    try:
        # Get provider as defined in user_config
        return IBMQ.get_provider(hub=provider_info['hub'], group=provider_info['group'], project=provider_info['project'])
    except:
        # Fallback if user has no access to the upper provider
        return IBMQ.get_provider(hub='ibm-q')

def get_device(network: Union[Network_DQNN, Network_QAOA], 
               simulator: bool = True,
               epoch: int = 0,
               do_calibration: bool = True,
               device_name: Optional[str] = None) -> Any:
    """
    If simulator == False:
        if device_name given:
            returns requested device
        else:
            returns the backend of the least busy functional device
    else:
        if device_name given:
            returns a noisy simulator corresponding to the given device_name
        else:
            returns the backend "qasm_simulator" without noise

    Args:
        network (Union[Network_DQNN, Network_QAOA]): Network object.
        simulator (bool, optional): If it should run on a simulator. Defaults to True.
        epoch (int, optional): Epoch of learning, used to identify calibrations. Defaults to 0.
        device_name (str, optional): Choose an execution device by name. Defaults to None.

    Returns:
        Any: Backend of the least busy device or simulator.
    """ 
    # Simulated device or raw simulator   
    if simulator:
        # device_name is "qasm_simulator(ibmq_athens)" after calling simulator=True and device_name="ibmq_athens"
        # -> check if "qasm_simulator(" is in the name to distinguish simulated real device from simulator
        if device_name and (not 'qasm_simulator' in device_name or "qasm_simulator(" in device_name):
            provider = get_provider()
            # if there is a bracket in the name, take the device name inside of it
            if "(" in device_name:
                device_name = device_name[15:-1]
            backend = provider.get_backend(device_name)
            simulated_backend = QasmSimulator.from_backend(backend)
            sd.save_calibration_info_from_backend(backend, epoch=epoch)
            return simulated_backend
        backend = Aer.get_backend('qasm_simulator')
        return backend
    
    # real device
    provider = get_provider()
    if (device_name):
        backend = provider.get_backend(device_name)
    else :
        min_qubits = network.required_qubits
        least_busy_device = least_busy(provider.backends(filters=lambda x: x.configuration().n_qubits >= min_qubits and 
                                not x.configuration().simulator and x.status().operational==True))
        backend = provider.get_backend(least_busy_device.name())
    sd.save_calibration_info_from_backend(backend, epoch=epoch)
    return backend
        

      
def number_of_loops(shots: int, shots_per_job: int = 2**13) -> Tuple[int, int]:
    """
    Helper function that enables to execute circuits with shots > 2^14.
    Simply calculates the loops necessary to reach shots with maximal number of shots_per_job.
    If shots <= 2^13 it simply returns loops=1 and shots as given

    Args:
        shots (int): Original and desired number of executed shots
        shots_per_job (int, optional): Maximally possible number of shots per job. Defaults to 2**13.

    Returns:
        Tuple[int, int]: Loops and number of shots per job
    """    
    loops = 1
    if (shots > shots_per_job):
        loops = shots//shots_per_job # makes int
        shots = shots_per_job
    return loops, shots

def make_diff_params(all_params: List[List[float]],
                     epsilon: float,
                     order_in_epsilon: int = 1) -> List[List[float]]:
    """
    Takes the list all_params that should look like [[param1, param2, ...]] and appends
    lists of all parameters with one parameter changed by
        + epsilon at a time and
        - epsilon at a time (if order_in_epsilon == 2)

    Args:
        all_params (List[List[float]]): Basis list of all original parameters that gets expanded.
        epsilon (float): Value by which parameter is changed by.
        order_in_epsilon (int, optional): Order of the derivative of the cost function. Defaults to 1.

    Returns:
        List[List[float]]: List of all parameter sets, inluding the original and the changed (one parameter at a time).
                            Its shape is (len(all_params)*order_in_epsilon + 1,len(all_params[0])).
    """    
    assert order_in_epsilon in [1,2], "Order in epsilon should be eiter 1 or 2."
    for sign in [+1, -1][:order_in_epsilon]:
        for i in range(len(all_params[0])):
            new_params = deepcopy(all_params[0])
            new_params[i] += sign * epsilon
            all_params.append(new_params)
    return all_params

def get_new_params_from_costs_trial(costs_trial: List[float], 
                                    all_params: List[List[float]], 
                                    eta: float, 
                                    epsilon: float,
                                    order_in_epsilon: int = 2,
                                    method: str = "gradient_descent",
                                    last_two_gradients: List[List[float]] = [],
                                    last_dist: List[float] = []) -> Tuple[List[List[float]],float, float,
                                                                                      List[List[float]], List[float]]:
    """
    Update all parameters according to the calculated costs_trial with variable order in epsilon
    and either with conjugate gradient or gradient descent.

    Args:
        costs_trial (List[float]): List of costs.
        all_params (List[List[float]]): List of a list of all network parameters
        eta (float): Learning rate.
        epsilon (float): Finite differentiation step.
        order_in_epsilon (int, optional): Order of the derivative of the cost function. Defaults to 2.
        method (str, optional): Gradient method. Defaults to 'gradient_descent'.
        last_two_gradients (List[List[float]], optional): Last two gradients, used for conjugate gradient method. Defaults to [].
        last_dist (List[List[float]], optional): Last distance vector, used for conjugate gradient method. Defaults to [].
        
    Returns:
        Tuple[List[List[float]],float, float, List[List[float]], List[float]]: Updated list of all network parameters,
            previous cost and standard deviation of the costs derivative. Two more lists for conjugate gradient.
    """    
    assert order_in_epsilon in [1,2], "Order in epsilon should be eiter 1 or 2."

    # Calculate gradient
    cost_old = costs_trial[0]
    if (order_in_epsilon == 1):
        cost_diffs = (costs_trial[1:len(costs_trial)] - np.full_like(costs_trial[1:len(costs_trial)],1)*cost_old)/epsilon # only + epsilon
    elif (order_in_epsilon == 2):
        # (C(x+epsilon) - C(x-epsilon)) / 2epsilon
        cost_diffs = (np.subtract(costs_trial[1:(len(costs_trial)-1)//2+1], costs_trial[(len(costs_trial)-1)//2+1:]))/(2*epsilon)
    cost_diffs = np.array(cost_diffs, dtype=np.float)
    
    # Calculate new parameters with conjugate gradient or gradient descent
    if method == 'conjugate_gradient':
        # Conjugate gradient method
        if len(last_two_gradients) < 2:
            # First two epochs
            dist = eta * (-np.array(cost_diffs))
        else:
            # After first two epochs
            beta_PR = np.dot(last_two_gradients[1], np.subtract(last_two_gradients[1],last_two_gradients[0])) / np.dot(last_two_gradients[0],last_two_gradients[0])
            beta = max(0.0, min(beta_PR, 1.0)) if cost_old < 0.99 else 0
            dist = -np.float(eta) * cost_diffs + beta * last_dist
        all_params = deepcopy([all_params[0] - dist])
        last_dist = dist
        # Update last two gradients and last step
        last_two_gradients.append(cost_diffs)
        return all_params, cost_old, np.std(cost_diffs), last_two_gradients[-2:], last_dist
    # gradient descent method
    else:
        all_params = deepcopy([all_params[0] + eta * np.array(cost_diffs)])
        return all_params, cost_old, np.std(cost_diffs), [], []

def train_network(network: Union[Network_DQNN, Network_QAOA],
                  training_pairs: List[List[np.ndarray]],
                  device_name: str,
                  epochs: int = 10,
                  epsilon: float = 0.25,
                  eta: float = 0.5,
                  simulator: bool = True,
                  shots: int = 2**13,
                  order_in_epsilon: int = 2,
                  gradient_method: str = "gradient_descent",
                  validation_step: Optional[int] = 5,
                  validation_pairs: Optional[List[List[np.ndarray]]] = None) -> Tuple[List[float],List[List[object]]]:
    """
    Trains the given network for the given amount of epochs.
    Simultaneously calculates an identity cost for comparison and optionally a validation cost.
    Plots the parameters and cost for each epoch.

    Args:
        network (Union[Network_DQNN, Network_QAOA]): Network object.
        training_pairs (List[List[np.ndarray]]): Training pairs.
        device_name (str): Choose a specific IBMQ device by name. All epochs will be executed by the given device.
        epochs (int, optional): Number of learning epochs. Defaults to 10.
        epsilon (float, optional): Epsilon of the cost derivative. Defaults to 0.1.
        eta (float, optional): Learning rate. Defaults to 0.1.
        simulator (bool, optional): Whether the simulator should be used. Defaults to True.
        shots (int, optional): How many times the device should repeat its measurement. Defaults to 2^13.
        order_in_epsilon (int, optional): Order of the derivative of the cost function. Defaults to 2.
        gradient_method (str, optional): Used gradient method. Defaults to 'gradient_descent'.
        validation_step (int, optional): After every validation_step's epoch a validation cost is calculated and plotted. Only if validation_pairs is not None. Defaults to 5.
        validation_pairs (List[List[np.ndarray]]): Validation pairs. Only used if validation_step is not None. Defaults to None.

    Returns:
        Tuple[List[float],List[List[object]]]: Updated network parameters and plot list of all params per epoch.
    """
    start_time = time()

    # Define BOOKKEEPING lists
    all_params_epochs = []
    plot_list_cost: List[List[Union[Union[int,float],float]]] = []
    plot_list_val: List[List[Union[Union[int,float],float]]] = []
    plot_list_std_cost_diffs: List[List[Union[Union[int,float],float]]] = []
    # Lists for conjugate gradient
    last_two_gradients: List[List[float]] = []
    last_dist: List[float] = []
    
    # Init params
    all_params = [network.params]
    print("PROGRESS |" + "-"*50 + "|\n" + " "*9 + "|", end="")
    for epoch in np.arange(epochs+1):
        # print("===== Epoch: {} =====".format(epoch))
        if int(100 * epoch/epochs) % 2  == 0:
            print("#", end="")
        all_params_epochs.append([epoch, deepcopy(all_params[0])])
        
        # Generate parameter sets to calculate the trial steps
        all_params = make_diff_params(all_params, epsilon, order_in_epsilon=order_in_epsilon) if (epoch < epochs) else [all_params[0]]
        circuits = [] 
        # get training cost circuits
        circuits.append(generate_circuits(network, all_params, "tp_circuits", shots, draw_circ=(epoch==0)))
        # Only execute in frequency of validation_step and for given validation_pairs
        if validation_step and (validation_pairs is not None):
            if epoch % validation_step == 0 or epoch == epochs:
                # get validation cost circuits
                circuits.append(generate_circuits(network, all_params[0], "vp_circuits", shots, draw_circ=False))
        # get identity and fidelity cost circuits
        circuits.append(generate_circuits(network, circuits_type="id_circuits", shots=shots, draw_circ=(epoch==0)))
        circuits.append(generate_circuits(network, circuits_type="fid_circuits", shots=shots, draw_circ=(epoch==0)))
        
        # execute circuits and divide results into training, validation, identity, fidelity
        t_costs, v_cost, id_cost, f_cost = execute_circuits(network, circuits, simulator, device_name, epoch, shots)
        
        # Save validation cost to list
        if not np.isnan(v_cost[0]): plot_list_val.append([epoch,v_cost[0]])
        
        if (epoch < epochs):
            # Use gradient method to calculate the next parameters
            all_params, cost_old, std_cost_diffs, last_two_gradients, last_dist = \
                get_new_params_from_costs_trial(t_costs, all_params, eta, epsilon, 
                                                order_in_epsilon=order_in_epsilon, method=gradient_method, 
                                                last_two_gradients=last_two_gradients, last_dist=last_dist)
            plot_list_cost.append([epoch,cost_old])
            plot_list_std_cost_diffs.append([epoch, std_cost_diffs])
        else: 
            plot_list_cost.append([epoch,t_costs[0]])  
        
        # Save and plot parameters, execution device, costs, and standard deviation of gradient
        sd.save(network=network, all_params_epochs=all_params_epochs, plot_list_cost=plot_list_cost, plot_list_val=plot_list_val, plot_list_id=[epoch, id_cost[0], f_cost[0]], plot_list_std_cost_diffs=plot_list_std_cost_diffs)
        
        # If simulator == True: terminate training if learning converges (training and validation cost)
        if validation_step and (validation_pairs is not None):
            if not np.isnan(id_cost[0]): identity_cost = id_cost[0]
            if not np.isnan(v_cost[0]): validation_cost = v_cost[0]
            if simulator and (epoch > epochs//4 and cost_old > 0.95 * identity_cost or validation_cost > 0.9 * identity_cost):
                print('Termination condition')
                if check_termination_condition(np.asarray(plot_list_cost)[:,1], np.asarray(plot_list_val)[:,1], val_step=validation_step, epoch_range=epochs//20, delta_cost=0.005*identity_cost):
                    break
    print("|", end="\n")
    runtime = time() - start_time
    sd.save_execution_info(cost=round(plot_list_cost[-1][1],3), validation=round(plot_list_val[-1][1],3), runtime_in_h=round(runtime/3600,3))
    return all_params[0], all_params_epochs

def check_termination_condition(cost_list: List[float],
                                val_list: List[float],
                                delta_cost: float = 0.01,
                                epoch_range: int = 50,
                                val_step: Optional[int] = 5) -> bool:
    """
    Checks if training is saturated, i.e. if both training cost and validation cost converge.

    Args:
        cost_list (List[float]): List of all training costs.
        val_list (List[float]): List of all validation costs.
        delta_cost (float, optional): Termination limit which must not be exceeded. Defaults to 0.01.
        epoch_range (int, optional): Size of the two sets that are compared. Defaults to 50.
        val_step (Optional[int], optional): validation_step used in train_network(). Defaults to None.

    Returns:
        bool: If true, the network training is terminated.
    """    
    if not val_step: return False
    if epoch_range < len(cost_list)/2 or epoch_range//val_step < len(val_list)/2: return False
    # Next to last set of validation costs with length epoch_range
    prev_val_avg = np.average(val_list[-2*(epoch_range//val_step):-epoch_range//val_step])
    # Last set of validation costs with length epoch_range
    cur_val_avg = np.average(val_list[-epoch_range//val_step:])
    # Compare validation costs
    if abs(cur_val_avg - prev_val_avg) < delta_cost:
        # Next to last set of training costs with length epoch_range
        prev_cost_avg = np.average(cost_list[-2*epoch_range:-epoch_range])
        # Last set of training costs with length epoch_range
        cur_cost_avg = np.average(cost_list[-epoch_range:])
        # Compare training costs
        if cur_cost_avg - prev_cost_avg < delta_cost:
            return True
    return False

if __name__ == "__main__":

    logger.info('Running DeepQNN.py as main...\n')
    